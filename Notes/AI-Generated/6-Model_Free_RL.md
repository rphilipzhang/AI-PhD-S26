# Model-Free Reinforcement Learning

**DOTE 6635: Artificial Intelligence for Business Research (Spring 2026)**

**Instructor: Renyu (Philip) Zhang**

## Abstract

This article provides a comprehensive introduction to model-free reinforcement learning (RL), a paradigm where agents learn optimal behavior through direct interaction with the environment without requiring explicit knowledge of transition dynamics or reward functions. The content is based on the lecture slides from the course "DOTE 6635: Artificial Intelligence for Business Research" and is supplemented with additional explanations and references to foundational literature. We begin by examining the core challenges of RL and the distinction between model-based and model-free approaches. Subsequently, we delve into Monte Carlo (MC) methods for policy evaluation and iteration. Finally, we explore Temporal-Difference (TD) learning, a cornerstone technique that combines ideas from MC methods and dynamic programming, and compare the bias-variance trade-offs between these two fundamental approaches.

## 1. Introduction to Model-Free Reinforcement Learning

### 1.1. Core Challenges of Reinforcement Learning

Before diving into specific algorithms, it is essential to understand the fundamental challenges that any RL system must address:

**Temporal Credit Assignment:** In sequential decision-making, rewards may arrive long after the actions that caused them. The challenge is determining which earlier actions deserve "credit" or "blame" for outcomes that occur later. For instance, in a game of chess, the winning move at the end of the game is a consequence of many strategic decisions made throughout the match.

**Exploration:** An agent must balance between exploiting known good actions and exploring potentially better alternatives. How can an agent efficiently explore to gain information about the environment without sacrificing too much reward?

**Generalization:** How can policies learned in one environment or set of states generalize to new, unseen situations? This is particularly important when the state space is vast or continuous.

### 1.2. Two Paradigms: RL for Planning vs. RL for Learning

Reinforcement learning manifests in two distinct paradigms, each with different assumptions and challenges:

**RL to Solve a Large Planning Problem:**
- Applications include AlphaGo, AlphaStar, simulated robotics, LLM coding, and LLM math reasoning.
- The transition dynamics are *known* (or can be simulated), but the state space is astronomically large.
- Data is collected by running a simulator.
- RL has proven highly successful in this domain.

**RL to Solve a Learning Problem:**
- Applications include adaptive medical treatment, deep research agents, and AI scientists.
- Transition dynamics are *unknown*, rewards are *unknown*, and there are too many states.
- Data must be collected by interacting with the real environment.
- This paradigm holds great potential but remains largely unrealized.

### 1.3. From Model-Based to Model-Free RL

In previous discussions of Markov Decision Processes (MDPs), we assumed complete knowledge of four critical components:

- **Action Space** $\mathcal{A}$: The set of all possible actions.
- **State Space** $\mathcal{S}$: The set of all possible states.
- **Transition Matrix** $P$: The probability of transitioning from one state to another given an action.
- **Reward Function** $R$: The immediate reward received after taking an action in a state.

With this complete information, algorithms like value iteration and policy iteration can compute the optimal policy through dynamic programming. However, in many real-world scenarios, the transition matrix $P$ and reward function $R$ are unknown. **Model-free RL** addresses this challenge by learning directly from experience without explicitly modeling the environment dynamics.

## 2. Monte Carlo Methods

### 2.1. The Monte Carlo Approach

Monte Carlo (MC) methods represent the most intuitive approach to model-free learning. The core idea is elegantly simple: **the value of a state equals the expected cumulative reward starting from that state, so we can estimate it by averaging the actual returns observed from many sample episodes.**

Key characteristics of MC methods:

- **Model-free:** No knowledge of MDP transitions or rewards is required.
- **Experience-based:** Learning occurs directly from episodes of interaction with the environment.
- **Episodic:** All episodes must terminate (reach a terminal state).
- **Simple estimation:** The value function is approximated by the empirical mean return.

### 2.2. MC Policy Evaluation

The objective of MC policy evaluation is to estimate the value function $v_\pi$ under a given policy $\pi$. Consider an episode generated by following policy $\pi$:

$$ S_0, A_0, R_0, \ldots, S_T \sim \pi $$

The return $G_t$ from time step $t$ is defined as:

$$ G_t = R_{t+1} + \gamma R_{t+2} + \ldots + \gamma^{T-1} R_T $$

The value function is then:

$$ v_\pi(s) = \mathbb{E}_\pi [G_t \mid S_t = s] $$

The Monte Carlo idea is to use the **empirical mean return** to approximate the expected return.

### 2.3. First-Visit vs. Every-Visit MC

Two variants of MC policy evaluation exist, differing in how they handle states that appear multiple times within an episode:

**First-Visit MC Policy Evaluation:**

The value $v_\pi(s)$ is estimated by averaging returns following only the *first* visit to state $s$ in each episode.

```
Initialize:
    N (counter), N(s) ← 0 for all s ∈ S
    Returns(s) ← empty list, for all s ∈ S

Repeat:
    Generate an episode following policy π
    For each distinct s appearing in the episode:
        G ← return following the first occurrence of s
        N(s) ← N(s) + 1
        Returns(s) ← Returns(s) + G

Output:
    For each distinct s:
        V(s) = Returns(s) / N(s)  →  v_π(s) as N(s) → ∞
```

> **Property:** First-visit MC evaluation is **unbiased**, and it converges to the true value function as the number of episodes approaches infinity.

**Every-Visit MC Policy Evaluation:**

The value $v_\pi(s)$ is estimated by averaging returns following *every* visit to state $s$ in a set of episodes.

```
Initialize:
    N (counter), N(s) ← 0 for all s ∈ S
    Returns(s) ← empty list, for all s ∈ S

Repeat:
    Generate an episode following policy π
    For each s appearing in the episode:
        G ← return following this occurrence of s
        N(s) ← N(s) + 1
        Returns(s) ← Returns(s) + G

Output:
    For each distinct s:
        V(s) = Returns(s) / N(s)  →  v_π(s) as N(s) → ∞
```

> **Property:** Every-visit MC evaluation is **biased** (because returns from the same episode are correlated), but it still converges to the true value function. It typically achieves a **smaller mean squared error (MSE)** because the effective sample size is much larger.

**Practical Note:** If there is an absorbing (terminal) state, the episode length is well-defined. Otherwise, one must specify a finite horizon $T$, which should be larger when the discount factor $\gamma$ is larger (to capture more of the discounted future rewards).

### 2.4. Incremental MC Policy Evaluation

Rather than storing all returns and computing averages at the end, we can update estimates incrementally. This approach reduces memory usage and is computationally more efficient.

The key insight comes from the incremental formula for computing means:

$$ \mu_k = \frac{1}{k} \sum_{j=1}^{k} x_j = \frac{1}{k} \left( x_k + \sum_{j=1}^{k-1} x_j \right) = \frac{1}{k} (x_k + (k-1)\mu_{k-1}) = \mu_{k-1} + \frac{1}{k}(x_k - \mu_{k-1}) $$

This can be interpreted as: **new estimate = old estimate + step size × (observation - old estimate)**.

Applying this to MC policy evaluation:

$$ N(S_t) \leftarrow N(S_t) + 1 $$

$$ V(S_t) \leftarrow V(S_t) + \frac{1}{N(S_t)} (G_t - V(S_t)) $$

**Handling Non-Stationary Environments:**

The incremental update enables adaptation to non-stationary environments where the underlying MDP may change over time. By replacing the decreasing step size $1/N(S_t)$ with a constant step size $\alpha$, we obtain:

$$ V(S_t) \leftarrow V(S_t) + \alpha (G_t - V(S_t)) $$

This formulation gives more weight to recent observations, allowing the algorithm to track changes in the environment. This is crucial for real-world applications where transition dynamics may evolve over time.

### 2.5. MC Policy Iteration

The goal of MC policy iteration is to use MC estimation to learn the **optimal policy**. The approach marries Monte Carlo evaluation with the policy iteration framework:

1. **Policy Evaluation:** Compute $v_\pi$ (or $Q_\pi$) via MC policy evaluation.
2. **Policy Improvement:** Improve the current policy with respect to the estimated action-value function $Q_\pi$.

The iteration proceeds as:

$$ \pi_0 \rightarrow Q^{\pi_0} \rightarrow \pi_1 \rightarrow Q^{\pi_1} \rightarrow \cdots \rightarrow \pi^{opt} \rightarrow Q^{\pi^{opt}} $$

**The Exploration Challenge:**

A critical challenge arises: under a deterministic policy, many state-action pairs may **never be visited**. If an action is never tried, we cannot estimate its value, and thus cannot determine if it might be better than the current policy's choice.

**Solution: ε-Greedy Exploration**

The ε-greedy strategy balances exploration and exploitation:

$$ \pi(a|s) = \begin{cases} \varepsilon/m + 1 - \varepsilon, & \text{if } a = \arg\max_a Q(s,a) \\ \varepsilon/m, & \text{otherwise} \end{cases} $$

where $m = |\mathcal{A}|$ is the number of actions. With probability $1-\varepsilon$, the agent selects the greedy action; with probability $\varepsilon$, it selects uniformly at random among all actions.

> **Theorem (ε-Greedy Policy Improvement):** The ε-greedy policy $\pi'$ with respect to $Q_\pi$ is an improvement over any ε-greedy policy $\pi$, i.e., $V_{\pi'}(s) \geq V_\pi(s)$ for all states $s$.

## 3. Temporal-Difference Learning

### 3.1. The Central Idea of TD Learning

Temporal-Difference (TD) learning is arguably the most important and novel contribution of reinforcement learning. As Sutton and Barto eloquently state:

> "If one had to identify one idea as central and novel to RL, it would undoubtedly be temporal-difference (TD) learning."

TD learning combines the advantages of Monte Carlo methods and dynamic programming:

- Like MC, TD is **model-free** and learns directly from experience.
- Like dynamic programming, TD **bootstraps**—it updates estimates based on other estimates without waiting for a final outcome.
- TD can be applied to **both episodic and non-episodic (continuing)** settings.
- TD **updates a guess towards a guess**, using the Bellman equation structure.

### 3.2. From MC to TD: The Intuition

Recall the two equivalent formulations of the value function under policy $\pi$:

**Formulation 1 (MC perspective):**
$$ V^\pi(s) = \mathbb{E}^\pi[G_t | S_t = s] \tag{1} $$

where $G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \cdots$ is the complete return.

**Formulation 2 (Bellman equation perspective):**
$$ V^\pi(s) = \mathbb{E}^\pi[R_t + \gamma V^\pi(S_{t+1}) | S_t = s] \tag{2} $$

The Bellman operator for policy evaluation is:

$$ B^\pi V(s) = r(s, \pi(s)) + \gamma \sum_{s' \in S} p(s'|s, \pi(s)) V(s') $$

**Monte Carlo** uses Formulation (1): it waits until the end of an episode to observe $G_t$, then updates:

$$ V(S_t) \leftarrow V(S_t) + \alpha (G_t - V(S_t)) $$

**Temporal-Difference** uses Formulation (2): it immediately uses the observed reward and the current estimate of the next state's value:

$$ V(S_t) \leftarrow V(S_t) + \alpha (R_{t+1} + \gamma V(S_{t+1}) - V(S_t)) $$

TD can be thought of as a **bootstrap method** that updates the current estimate based on an existing estimate of the future.

### 3.3. The TD(0) Algorithm

TD(0) is the simplest temporal-difference learning algorithm:

**Key Definitions:**
- **TD Target:** $R_{t+1} + \gamma V(S_{t+1})$ — the estimated return using the immediate reward and the current value estimate.
- **TD Error:** $\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)$ — the difference between the TD target and the current estimate.

```
Input: π (policy to be evaluated), α (step size)
Initialize: V arbitrarily (e.g., V(s) = 0 for all s)

Repeat for each episode:
    Initialize state s
    Repeat for each step of the episode:
        a ← action given by π for s
        Take action a, observe reward r and next state s'
        V(s) ← V(s) + α[r + γV(s') - V(s)]
        s ← s'
    until s is a terminal state
```

The update rule can be written compactly as:

$$ V(S_t) \leftarrow V(S_t) + \alpha \cdot \delta_t $$

where $\delta_t$ is the TD error.

### 3.4. MC vs. TD: A Comprehensive Comparison

The choice between MC and TD methods involves fundamental trade-offs in terms of bias, variance, and applicability.

| Aspect | Monte Carlo | Temporal-Difference |
|--------|-------------|---------------------|
| **Update timing** | Must wait until episode ends | Updates after each step |
| **Learning mode** | From complete sequences | From incomplete sequences |
| **Environment type** | Episodic only | Episodic and continuing |
| **Target** | Actual return $G_t$ | Estimated return $R_t + \gamma V(S_{t+1})$ |

### 3.5. The Bias-Variance Trade-off

The distinction between MC and TD fundamentally reflects a bias-variance trade-off in estimation:

**Monte Carlo:**
- MC updates using $G_t$, which is an **unbiased estimate** of $v_\pi(S_t)$.
- MC has **high variance, zero bias**.
- The return $G_t$ depends on many random actions, transitions, and rewards throughout the episode.
- **Advantages:**
  - Good convergence properties even with function approximation (e.g., using deep neural networks).
  - Not sensitive to initial value estimates.
  - Simple and intuitive—does not require knowledge of the Bellman equation.
  - More efficient in non-Markov environments.

**Temporal-Difference:**
- TD updates using $R_t + \gamma V(S_{t+1})$, not $R_t + \gamma v_\pi(S_{t+1})$, which is a **biased estimate** of $v_\pi(S_t)$ (because $V$ is itself an estimate).
- TD has **much lower variance, some bias**.
- The TD target depends on only one random action, transition, and reward.
- **Advantages:**
  - More data-efficient than MC because it exploits the Markov property and the Bellman equation structure.
  - TD(0) provably converges to the true value function (though not always with function approximation).
- **Disadvantages:**
  - More sensitive to initial value estimates.
  - Convergence guarantees weaker with function approximation.

### 3.6. Empirical Comparison: Random Walk Example

The difference between MC and TD is vividly illustrated by the classic random walk example. Consider a Markov chain with states A, B, C, D, E arranged linearly:

```
[Terminal] ← A ← B ← C ← D ← E → [Terminal]
  (reward 0)       (start)        (reward 1)
```

The agent starts at state C and moves randomly left or right with equal probability. Reaching the left terminal yields reward 0; reaching the right terminal yields reward 1.

Experimental results show that:
- **TD converges faster** than MC across all learning rates.
- **TD achieves lower RMS error** after the same number of episodes.
- The optimal learning rate for TD is larger than for MC, reflecting TD's lower variance.

This empirical advantage of TD stems from its ability to learn incrementally and leverage the Markov structure of the problem.

## 4. Conclusion

Model-free reinforcement learning provides powerful tools for learning optimal behavior when the environment dynamics are unknown. Monte Carlo methods offer simplicity and unbiasedness by learning from complete episodes, while Temporal-Difference learning provides data efficiency and online learning capability through bootstrapping.

The choice between MC and TD depends on the specific problem characteristics:
- Use **MC** when episodes are short, the environment may be non-Markov, or when unbiasedness is critical.
- Use **TD** when learning must occur online, episodes are long or infinite, or when data efficiency is paramount.

For business researchers, these model-free methods open doors to applications where environment models are unavailable or too complex to specify—from dynamic pricing and inventory management to personalized recommendations and adaptive clinical trials. The fundamental trade-offs between bias and variance, exploration and exploitation, and sample efficiency and computational cost remain central considerations in applying these methods to real-world problems.

## References

[1] Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction* (2nd ed.). MIT Press. [http://incompleteideas.net/book/the-book-2nd.html](http://incompleteideas.net/book/the-book-2nd.html)

[2] Silver, D. (2015). *Lectures on Reinforcement Learning*. University College London. [https://www.davidsilver.uk/teaching/](https://www.davidsilver.uk/teaching/)

[3] Watkins, C. J., & Dayan, P. (1992). *Q-learning*. Machine Learning, 8(3-4), 279-292.

[4] Tsitsiklis, J. N., & Van Roy, B. (1997). *An analysis of temporal-difference learning with function approximation*. IEEE Transactions on Automatic Control, 42(5), 674-690.

[5] Singh, S. P., & Sutton, R. S. (1996). *Reinforcement learning with replacing eligibility traces*. Machine Learning, 22(1-3), 123-158.

[6] Mnih, V., Kavukcuoglu, K., Silver, D., et al. (2015). *Human-level control through deep reinforcement learning*. Nature, 518(7540), 529-533. [https://www.nature.com/articles/nature14236](https://www.nature.com/articles/nature14236)

[7] Bertsekas, D. P., & Tsitsiklis, J. N. (1996). *Neuro-Dynamic Programming*. Athena Scientific.
